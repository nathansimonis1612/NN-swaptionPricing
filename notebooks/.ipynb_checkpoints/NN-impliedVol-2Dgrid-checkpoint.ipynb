{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - The objective of this notebook is to train a neural network to learn how to price a swaption for a given underlying model. As input to the neural network, we will consider the zero rate curve and the model parameters.  \n",
    "\n",
    "#### - What we want to \"predict\" is a Bachelier implied volatility grid for a given strike and expiry of the option. \n",
    "\n",
    "#### - The benefit of this exercise is that once the neural network is trained, we have an approximation of the implied volality extremely quickly.  In a calibration exercise, we can then replace the underlying pricing model which can take a long time to compute and replace it with the neural network.\n",
    "\n",
    "**Reference**: Horvath, B., Muguruza, A. and Tomas, M., 2019. Deep Learning Volatility. Available at SSRN 3322085."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations: 72\n",
      "9 8\n"
     ]
    }
   ],
   "source": [
    "# Note down the grid we shall be using.\n",
    "\n",
    "strike = np.arange(-0.02, 0.025, step=0.005)\n",
    "optMats = [0.5, 1, 2, 5, 10, 15, 20, 30]\n",
    "grid = {\n",
    "    'chgStrike' : strike,\n",
    "    'optMat' : optMats\n",
    "}\n",
    "keys, values = zip(*grid.items())\n",
    "combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "combinations = pd.DataFrame(combinations)\n",
    "print(\"Number of combinations:\",len(combinations))\n",
    "print(len(strike), len(optMats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "X = pd.read_csv(f\"data/2DpricingVector-input.csv.gz\")\n",
    "X[\"sigma\"] = np.maximum(X[\"sigma\"], 1e-12)\n",
    "y = pd.read_csv(f\"data/2DgridImpliedVolatility.csv.gz\")\n",
    "\n",
    "X.hist(figsize=(15,10), bins=50, color=\"black\");\n",
    "plt.savefig(\"plots/histogramInput.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data transformations:\n",
    "- We apply PCA on the curve and keep the first 4 components.  The curve is standardized before applying PCA on it\n",
    "- We scale the input data between -1 and 1\n",
    "- We standardize the implied volatility $\\frac{\\sigma - mean(\\sigma)}{std(\\sigma)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usePCA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Split train test\n",
    "TRAINING_SIZE = 0.8\n",
    "split = int(TRAINING_SIZE*X.shape[0])\n",
    "X_tr, X_te = X[:split].reset_index(drop=True), X[split:].reset_index(drop=True)\n",
    "y_tr, y_te = y[:split].reset_index(drop=True), y[split:].reset_index(drop=True)\n",
    "\n",
    "# Extract curve\n",
    "curveCols = X.columns[X.columns.str.contains(\"yrs\")]\n",
    "curve_tr, curve_te = X_tr[curveCols].copy(), X_te[curveCols].copy()\n",
    "\n",
    "if usePCA:\n",
    "    # Std scaler\n",
    "    scaler = StandardScaler()\n",
    "    curve_tr_sc, curve_te_sc = scaler.fit_transform(curve_tr), scaler.transform(curve_te)\n",
    "    # PCA\n",
    "    comps=3\n",
    "    pca = PCA(n_components=comps)\n",
    "    curve_tr_pca, curve_te_pca = pca.fit_transform(curve_tr_sc), pca.transform(curve_te_sc)\n",
    "    curve_tr_pca, curve_te_pca = pd.DataFrame(curve_tr_pca), pd.DataFrame(curve_te_pca)\n",
    "    colNames = [f\"comp_{x}\" for x in range(0, comps)]\n",
    "    curve_tr_pca.columns, curve_te_pca.columns = colNames, colNames\n",
    "    # Merge data \n",
    "    X_tr_clean = pd.concat([curve_tr_pca, X_tr.drop(columns=curveCols)], axis=1)\n",
    "    X_te_clean = pd.concat([curve_te_pca, X_te.drop(columns=curveCols)], axis=1)\n",
    "    # Scale between -1 and 1\n",
    "    rng = MinMaxScaler(feature_range=(-1, 1))\n",
    "    X_tr_clean, X_te_clean = rng.fit_transform(X_tr_clean), rng.transform(X_te_clean)\n",
    "else:\n",
    "    rng = rng = MinMaxScaler(feature_range=(-1, 1))\n",
    "    X_tr_clean, X_te_clean = rng.fit_transform(X_tr), rng.transform(X_te)\n",
    "\n",
    "scaleY = StandardScaler()\n",
    "y_tr_clean, y_te_clean = scaleY.fit_transform(y_tr), scaleY.transform(y_te) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "keras.backend.set_floatx('float64')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Root mean square error loss\"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Initialize model\"\"\"\n",
    "    input1 = keras.layers.Input(shape=(X_tr_clean.shape[1],))\n",
    "    x1 = keras.layers.Dense(32,activation = 'elu')(input1)\n",
    "    x2=keras.layers.Dense(32,activation = 'elu')(x1) \n",
    "    x3=keras.layers.Dense(32,activation = 'elu')(x2) \n",
    "    x4=keras.layers.Dense(32, activation = 'elu')(x3)\n",
    "    output1=keras.layers.Dense(y_tr_clean.shape[1],activation = 'linear')(x4)\n",
    "\n",
    "    model = keras.models.Model(inputs=input1, outputs=output1)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "### Uncomment if you want to train the NN again.  No need to do so as it has been save in .h5 file.\n",
    "#model = create_model() \n",
    "#model.compile(loss=rmse, optimizer=\"adam\")\n",
    "#\n",
    "#earlyStop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "#lrReduce = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", verbose=1, patience=8)\n",
    "#\n",
    "#history = model.fit(\n",
    "# x=X_tr_clean,\n",
    "# y=y_tr_clean,\n",
    "# batch_size=32,\n",
    "# epochs=500,\n",
    "# validation_split=0.2,\n",
    "# callbacks=[earlyStop, lrReduce]\n",
    "#)\n",
    "#\n",
    "#model.save_weights(f\"savedNN/model-impliedVol-pca{usePCA}.h5\")\n",
    "#pickle.dump(history.history, open(f\"savedNN/savedHistory-impliedVol-pca{usePCA}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pickle.load(open(f\"savedNN/savedHistory-impliedVol-pca{usePCA}.pkl\", \"rb\"))\n",
    "# Plot training metrics\n",
    "plt.plot(history[\"loss\"], label=\"Training\", color=\"black\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Validation\", color=\"orange\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss Metric\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/NN-learningHist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = create_model()\n",
    "model.load_weights(f\"savedNN/model-impliedVol-pca{usePCA}.h5\")\n",
    "\n",
    "# Compute predictions\n",
    "pred_oos = model.predict(X_te_clean)\n",
    "pred_oos = scaleY.inverse_transform(pred_oos)\n",
    "\n",
    "def r2score(y_true, y_pred):\n",
    "    SS_res =  np.sum(np.square( y_true-y_pred ))\n",
    "    SS_tot = np.sum(np.square( y_true - np.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot) )\n",
    "def rmse_np(y_true, y_pred):\n",
    "    return np.sqrt( np.mean( (y_true - y_pred)**2 ) )\n",
    "\n",
    "rmse_score = rmse_np(y_te.values, pred_oos)\n",
    "print(\"RMSE:\",rmse_score)\n",
    "\n",
    "r2 = r2score(y_te.values, pred_oos)\n",
    "print(\"R^2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 10000 * np.mean(np.abs(pred_oos - y_te.values), axis=0) # Absolute error in bps\n",
    "errDf = pd.DataFrame(error.reshape(len(strike), len(optMats)).T, index=optMats, columns=np.around(strike,4))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plt.imshow(errDf, cmap=\"Oranges\")\n",
    "plt.colorbar()\n",
    "ax.set_xticks(range(0,len(strike)))\n",
    "ax.set_xticklabels(np.around(strike,3))\n",
    "ax.set_yticks(range(0, len(optMats)))\n",
    "ax.set_yticklabels(optMats)\n",
    "ax.set_xlabel(r\"$\\Delta Strike$\")\n",
    "ax.set_ylabel(\"optMat\")\n",
    "plt.title(\"Mean Absolute Error (bps)\")\n",
    "plt.savefig(\"plots/matrixMeanAbsError-bps.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorAxis = 10000 * np.mean(np.abs(pred_oos - y_te.values),axis=1)\n",
    "fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "ax1.scatter(X_te[\"sigma\"], errorAxis, s=0.1, c=\"black\")\n",
    "ax1.set_ylim(0, 5)\n",
    "ax1.set_xlabel(\"HW1F volatility\")\n",
    "ax1.set_ylabel(\"Mean Absolute Error (bps)\")\n",
    "plt.title(\"\")\n",
    "plt.savefig(\"plots/errorHW1Fvol.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Error on a subset \n",
    "\n",
    "subsetIdx = X_te[(X_te[\"sigma\"]>0.005) & (X_te[\"sigma\"]<0.02)].index\n",
    "X_subset = X_te_clean[subsetIdx]\n",
    "y_subset = y_te.iloc[subsetIdx]\n",
    "\n",
    "pred_subset = model.predict(X_subset)\n",
    "pred_subset = scaleY.inverse_transform(pred_subset)\n",
    "\n",
    "error = 10000 * np.mean(np.abs(pred_subset - y_subset.values), axis=0) # Absolute error in bps\n",
    "errDf = pd.DataFrame(error.reshape(len(strike), len(optMats)).T, index=optMats, columns=np.around(strike,4))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plt.imshow(errDf, cmap=\"Oranges\")\n",
    "plt.colorbar()\n",
    "ax.set_xticks(range(0,len(strike)))\n",
    "ax.set_xticklabels(np.around(strike,3))\n",
    "ax.set_yticks(range(0, len(optMats)))\n",
    "ax.set_yticklabels(optMats)\n",
    "ax.set_xlabel(r\"$\\Delta Strike$\")\n",
    "ax.set_ylabel(\"optMat\")\n",
    "plt.title(\"Mean Absolute Error (bps)\")\n",
    "plt.savefig(\"plots/trimSigma-matrixMeanAbsError-bps.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = X_te[(X_te[\"sigma\"] < 0.016 )& (X_te[\"sigma\"] > 0.014) & (X_te[\"swapMat\"]==20)].index.values\n",
    "maturities = np.array([7/360, 30/360, 90/360, 180/360, 1, 3, 5, 10, 15, 20, 30, 50])\n",
    "\n",
    "bpsConversion = 10000\n",
    "# Plot metrics for a single sample\n",
    "randomIdx = np.random.choice(pool)\n",
    "sample_y = y_te.iloc[randomIdx].values * bpsConversion\n",
    "sample_pred = pred_oos[randomIdx] * bpsConversion\n",
    "\n",
    "sigma, swapMat = X_te.loc[randomIdx, 'sigma'], X_te.loc[randomIdx, 'swapMat']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(maturities, X_te[curveCols].iloc[randomIdx], color=\"black\")\n",
    "ax.title.set_text(\"Sampled Yield Curve\")\n",
    "ax.set_ylabel(\"YTM\")\n",
    "ax.set_xlabel(\"TTM\")\n",
    "plt.savefig(\"plots/sampleYieldCurve.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(sample_y, label=\"hw1f\", color=\"black\")\n",
    "ax1.plot(sample_pred, label=\"NN\", ls=\"--\", color=\"orange\")\n",
    "ax1.legend()\n",
    "ax1.title.set_text(\"Normal Volatility for one pair \"+r\"$(x_n, y_n)$\"+f\"\\nHW1F sigma:{sigma:.3f}|Swap maturity:{swapMat}\")\n",
    "ax1.set_xlabel(\"Combination (Strike, optMat)\")\n",
    "ax1.set_ylabel(\"Normal Volatility (bps)\")\n",
    "ax2.plot((sample_y - sample_pred), color=\"black\")\n",
    "ax2.title.set_text(\"Spread (bps)\")\n",
    "ax2.axhline(0, ls=\"--\")\n",
    "ax2.set_xlabel(\"Combination (Strike, optMat)\")\n",
    "ax2.set_ylabel(r\"$\\delta$ (bps)\")\n",
    "\n",
    "plt.savefig(\"plots/errorOneSample-NN_HW1F.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW1F implied smile\n",
    "plt.figure(1, figsize=(25,20))\n",
    "for ii, optMat in enumerate(combinations[\"optMat\"].unique()):\n",
    "    lkp = combinations[(combinations[\"optMat\"]==optMat)]\n",
    "    y_vol, pred_vol = sample_y[lkp.index], sample_pred[lkp.index]\n",
    "    \n",
    "    plt.subplot(4, 4, ii+1)\n",
    "    plt.plot(lkp[\"chgStrike\"], y_vol,label=\"hw1f\", color=\"black\")\n",
    "    plt.plot(lkp[\"chgStrike\"], pred_vol, label=\"NN\", ls=\"--\", color=\"orange\")\n",
    "    plt.xlabel(r\"$\\Delta Strike$\")\n",
    "    plt.ylabel(\"Normal Vol\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"optMat:{optMat}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/hw1fSmile.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time dependency\n",
    "plt.figure(1, figsize=(25, 20))\n",
    "for ii, chgStrike in enumerate(combinations[\"chgStrike\"].unique()):\n",
    "    lkp = combinations[(combinations[\"chgStrike\"]==chgStrike)]\n",
    "    y_vol, pred_vol = sample_y[lkp.index], sample_pred[lkp.index]\n",
    "    \n",
    "    plt.subplot(5, 4, ii+1)\n",
    "    plt.plot(lkp[\"optMat\"], y_vol, label=\"hw1f\", color=\"black\")\n",
    "    plt.plot(lkp[\"optMat\"], pred_vol,label=\"NN\", ls=\"--\", color=\"orange\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Option Maturity\")\n",
    "    plt.ylabel(\"Normal Vol\")\n",
    "    plt.title(f\"chgStrike:{chgStrike:.3f}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/hw1fTimeDep.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
